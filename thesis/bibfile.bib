
@book{quinlan_c45,
author = {Quinlan, J. Ross},
title = {C4.5: Programs for Machine Learning},
year = {1993},
isbn = {1558602402},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {From the Publisher:Classifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. C4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies.This book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses.}
}

@BOOK{cart_1,
  author        = {L. Breiman and J. Friedman and R. Olshen and C. Stone},
  title         = {{Classification and Regression Trees}},
  publisher     = {Wadsworth and Brooks},
  address       = {Monterey, CA},
  year          = {1984},
  note          = {new edition \cite{cart93}?},
  remarks       = {cited in \cite{cslu:esca98mm, cslu:icslp98cronk, cstr:unitsel97} for CART, clustering, and decision trees},
  abstract      = {},
}

@BOOK{cart_2,
  author        = {Leo {Breiman} and J. H. {Friedman} and R. A. {Olshen} and C. J. {Stone}},
  title         = {Classification and Regression Trees},
  year          = {1984},
  publisher     = {Wadsworth Publishing Company},
  address       = {Belmont, California, U.S.A.},
  series        = {Statistics/Probability Series},
  isbn-hard     = {0534980546 (softcover)},
  isbn-soft     = {0534980538 (hardcover)},
}

@BOOK{cart_3,
  author        = {Leo Breiman and others},
  title         = {{Classification and Regression Trees}},
  publisher     = {Chapman \& Hall},
  address       = {New York},
  year          = {1984},
  pages         = {358},
  note          = {new edition of \cite{cart84}?},
  isbn          = {0-412-04841-8},
  url           = {http://www.crcpress.com/catalog/C4841.htm},
  amazon-url    = {http://www.amazon.de/exec/obidos/ASIN/0412048418},
  price         = {\$44.95, DM 83.26 EUR 42.57},
  remarks       = {\tbf},
  abstract      = {},
}

@book{hastie_elemstatlearn,
  added-at = {2008-05-16T16:17:42.000+0200},
  address = {New York, NY, USA},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  biburl = {https://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
  interhash = {d585aea274f2b9b228fc1629bc273644},
  intrahash = {f58afc5c9793fcc8ad8389824e57984c},
  keywords = {ml statistics},
  publisher = {Springer New York Inc.},
  series = {Springer Series in Statistics},
  timestamp = {2008-05-16T16:17:43.000+0200},
  title = {The Elements of Statistical Learning},
  year = 2001
}

@article{breiman_randomforests,
author = {Breiman, Leo},
title = {Random Forests},
year = {2001},
issue_date = {October 1 2001},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {45},
number = {1},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1010933404324},
doi = {10.1023/A:1010933404324},
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund &amp; R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
journal = {Mach. Learn.},
month = oct,
pages = {5–32},
numpages = {28},
keywords = {ensemble, regression, classification}
}

@article{zeileis_mob,
author = {Achim Zeileis and Torsten Hothorn and Kurt Hornik},
title = {Model-Based Recursive Partitioning},
journal = {Journal of Computational and Graphical Statistics},
volume = {17},
number = {2},
pages = {492-514},
year  = {2008},
publisher = {Taylor & Francis},
doi = {10.1198/106186008X319331},

URL = { 
        https://doi.org/10.1198/106186008X319331
    
},
eprint = { 
        https://doi.org/10.1198/106186008X319331
    
}

}

@Article{party_package,
title = {Unbiased Recursive Partitioning: A Conditional Inference
  Framework},
author = {Torsten Hothorn and Kurt Hornik and Achim Zeileis},
journal = {Journal of Computational and Graphical Statistics},
year = {2006},
volume = {15},
number = {3},
pages = {651--674},
}

@Manual{r_citation,
title = {R: A Language and Environment for Statistical Computing},
author = {{R Core Team}},
organization = {R Foundation for Statistical Computing},
address = {Vienna, Austria},
year = {2020},
url = {https://www.R-project.org/},
}

@Article{partykit_package,
title = {{partykit}: A Modular Toolkit for Recursive Partytioning
  in {R}},
author = {Torsten Hothorn and Achim Zeileis},
journal = {Journal of Machine Learning Research},
year = {2015},
volume = {16},
pages = {3905-3909},
url = {https://jmlr.org/papers/v16/hothorn15a.html},
}

@Article{ranger_package,
title = {{ranger}: A Fast Implementation of Random Forests for High
  Dimensional Data in {C++} and {R}},
author = {Marvin N. Wright and Andreas Ziegler},
journal = {Journal of Statistical Software},
year = {2017},
volume = {77},
number = {1},
pages = {1--17},
doi = {10.18637/jss.v077.i01},
}

@article{eddelbuettel_rcpp,
   author = {Dirk Eddelbuettel and Romain Francois},
   title = {Rcpp: Seamless R and C++ Integration},
   journal = {Journal of Statistical Software, Articles},
   volume = {40},
   number = {8},
   year = {2011},
   keywords = {},
   abstract = {The Rcpp package simplifies integrating C++ code with R. It provides a consistent C++ class hierarchy that maps various types of R objects (vectors, matrices, functions, environments, . . . ) to dedicated C++ classes. Object interchange between R and C++ is managed by simple, flexible and extensible concepts which include broad support for C++ Standard Template Library idioms. C++ code can both be compiled, linked and loaded on the fly, or added via packages. Flexible error and exception code handling is provided. Rcpp substantially lowers the barrier for programmers wanting to combine C++ code with R.},
   issn = {1548-7660},
   pages = {1--18},
   doi = {10.18637/jss.v040.i08},
   url = {https://www.jstatsoft.org/v040/i08}
}

@Article{quinlan_induction,
author={Quinlan, J. R.},
title={Induction of decision trees},
journal={Machine Learning},
year={1986},
month={Mar},
day={01},
volume={1},
number={1},
pages={81-106},
abstract={The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.},
issn={1573-0565},
doi={10.1007/BF00116251},
url={https://doi.org/10.1007/BF00116251}
}


@article{loh_trees_review,
author = {Loh, Wei-Yin},
title = {Fifty Years of Classification and Regression Trees},
journal = {International Statistical Review},
volume = {82},
number = {3},
pages = {329-348},
keywords = {Classification trees, regression trees, machine learning, prediction},
doi = {https://doi.org/10.1111/insr.12016},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12016},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12016},
abstract = {AbstractFifty years have passed since the publication of the first regression tree algorithm. New techniques have added capabilities that far surpass those of the early methods. Modern classification trees can partition the data with linear splits on subsets of variables and fit nearest neighbor, kernel density, and other models in the partitions. Regression trees can fit almost every kind of traditional statistical model, including least-squares, quantile, logistic, Poisson, and proportional hazards models, as well as models for longitudinal and multiresponse data. Greater availability and affordability of software (much of which is free) have played a significant role in helping the techniques gain acceptance and popularity in the broader scientific community. This article surveys the developments and briefly reviews the key ideas behind some of the major algorithms.},
year = {2014}
}


﻿@Article{ali_trees,
author={Ali, M. Abid
and Hickman, P. J.
and Clementson, A. T.},
title={The Application of Automatic Interaction Detection (AID) in Operational Research},
journal={Operational Research Quarterly (1970-1977)},
year={1975},
month={2021/06/14/},
publisher={Palgrave Macmillan Journals},
volume={26},
number={2},
pages={243-252},
abstract={[Automatic Interaction Detection (AID) is a multivariate statistical technique. This paper describes the algorithm and illustrates its application to a particular case study in the British fishing industry.]},
note={Full publication date: Jun., 1975},
issn={00303623},
doi={10.2307/3008458},
url={https://doi.org/10.2307/3008458},
url={http://www.jstor.org/stable/3008458}
}

@article{morgan_sonquist_trees,
author = { James N.   Morgan  and  John A.   Sonquist },
title = {Problems in the Analysis of Survey Data, and a Proposal},
journal = {Journal of the American Statistical Association},
volume = {58},
number = {302},
pages = {415-434},
year  = {1963},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.1963.10500855},

URL = { 
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1963.10500855
    
},
eprint = { 
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1963.10500855
    
}

}

@inbook{hawkins_AID, place={Cambridge}, title={AUTOMATIC INTERACTION DETECTION}, DOI={10.1017/CBO9780511897375.006}, booktitle={Topics in Applied Multivariate Analysis}, publisher={Cambridge University Press}, author={Hawkins, Douglas M. and Kass, Gordon V. and Hawkins, D. M.}, year={1982}, pages={269–302}}

@article{podgorelec_trees_medicine,
author = {Podgorelec, Vili and Kokol, Peter and Stiglic, Bruno and Rozman, Ivan},
year = {2002},
month = {11},
pages = {445-63},
title = {Decision Trees: An Overview and Their Use in Medicine},
volume = {26},
journal = {Journal of medical systems},
doi = {10.1023/A:1016409317640}
}

@article{hothorn_ctree,
author = {Torsten Hothorn and Kurt Hornik and Achim Zeileis},
title = {Unbiased Recursive Partitioning: A Conditional Inference Framework},
journal = {Journal of Computational and Graphical Statistics},
volume = {15},
number = {3},
pages = {651-674},
year  = {2006},
publisher = {Taylor & Francis},
doi = {10.1198/106186006X133933},

URL = { 
        https://doi.org/10.1198/106186006X133933
    
},
eprint = { 
        https://doi.org/10.1198/106186006X133933
    
}

}