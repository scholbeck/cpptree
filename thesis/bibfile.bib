
@book{quinlan_c45,
author = {Quinlan, J. Ross},
title = {C4.5: Programs for Machine Learning},
year = {1993},
isbn = {1558602402},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {From the Publisher:Classifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. C4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies.This book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses.}
}

@BOOK{cart_1,
  author        = {L. Breiman and J. Friedman and R. Olshen and C. Stone},
  title         = {{Classification and Regression Trees}},
  publisher     = {Wadsworth and Brooks},
  address       = {Monterey, CA},
  year          = {1984},
  note          = {new edition \cite{cart93}?},
  remarks       = {cited in \cite{cslu:esca98mm, cslu:icslp98cronk, cstr:unitsel97} for CART, clustering, and decision trees},
  abstract      = {},
}

@BOOK{cart_2,
  author        = {Leo {Breiman} and J. H. {Friedman} and R. A. {Olshen} and C. J. {Stone}},
  title         = {Classification and Regression Trees},
  year          = {1984},
  publisher     = {Wadsworth Publishing Company},
  address       = {Belmont, California, U.S.A.},
  series        = {Statistics/Probability Series},
  isbn-hard     = {0534980546 (softcover)},
  isbn-soft     = {0534980538 (hardcover)},
}

@BOOK{cart_3,
  author        = {Leo Breiman and others},
  title         = {{Classification and Regression Trees}},
  publisher     = {Chapman \& Hall},
  address       = {New York},
  year          = {1984},
  pages         = {358},
  note          = {new edition of \cite{cart84}?},
  isbn          = {0-412-04841-8},
  url           = {http://www.crcpress.com/catalog/C4841.htm},
  amazon-url    = {http://www.amazon.de/exec/obidos/ASIN/0412048418},
  price         = {\$44.95, DM 83.26 EUR 42.57},
  remarks       = {\tbf},
  abstract      = {},
}

@book{hastie_elemstatlearn,
  added-at = {2008-05-16T16:17:42.000+0200},
  address = {New York, NY, USA},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  biburl = {https://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
  interhash = {d585aea274f2b9b228fc1629bc273644},
  intrahash = {f58afc5c9793fcc8ad8389824e57984c},
  keywords = {ml statistics},
  publisher = {Springer New York Inc.},
  series = {Springer Series in Statistics},
  timestamp = {2008-05-16T16:17:43.000+0200},
  title = {The Elements of Statistical Learning},
  year = 2001
}

@article{breiman_randomforests,
author = {Breiman, Leo},
title = {Random Forests},
year = {2001},
issue_date = {October 1 2001},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {45},
number = {1},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1010933404324},
doi = {10.1023/A:1010933404324},
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund &amp; R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
journal = {Mach. Learn.},
month = oct,
pages = {5–32},
numpages = {28},
keywords = {ensemble, regression, classification}
}

@article{zeileis_mob,
author = {Achim Zeileis and Torsten Hothorn and Kurt Hornik},
title = {Model-Based Recursive Partitioning},
journal = {Journal of Computational and Graphical Statistics},
volume = {17},
number = {2},
pages = {492-514},
year  = {2008},
publisher = {Taylor & Francis},
doi = {10.1198/106186008X319331},

URL = { 
        https://doi.org/10.1198/106186008X319331
    
},
eprint = { 
        https://doi.org/10.1198/106186008X319331
    
}

}

@Article{party_package,
title = {Unbiased Recursive Partitioning: A Conditional Inference
  Framework},
author = {Torsten Hothorn and Kurt Hornik and Achim Zeileis},
journal = {Journal of Computational and Graphical Statistics},
year = {2006},
volume = {15},
number = {3},
pages = {651--674},
}

@Manual{r_citation,
title = {R: A Language and Environment for Statistical Computing},
author = {{R Core Team}},
organization = {R Foundation for Statistical Computing},
address = {Vienna, Austria},
year = {2020},
url = {https://www.R-project.org/},
}

@Article{partykit_package,
title = {{partykit}: A Modular Toolkit for Recursive Partytioning
  in {R}},
author = {Torsten Hothorn and Achim Zeileis},
journal = {Journal of Machine Learning Research},
year = {2015},
volume = {16},
pages = {3905-3909},
url = {https://jmlr.org/papers/v16/hothorn15a.html},
}

@Article{ranger_package,
title = {{ranger}: A Fast Implementation of Random Forests for High
  Dimensional Data in {C++} and {R}},
author = {Marvin N. Wright and Andreas Ziegler},
journal = {Journal of Statistical Software},
year = {2017},
volume = {77},
number = {1},
pages = {1--17},
doi = {10.18637/jss.v077.i01},
}

@article{eddelbuettel_rcpp,
   author = {Dirk Eddelbuettel and Romain Francois},
   title = {Rcpp: Seamless R and C++ Integration},
   journal = {Journal of Statistical Software, Articles},
   volume = {40},
   number = {8},
   year = {2011},
   keywords = {},
   abstract = {The Rcpp package simplifies integrating C++ code with R. It provides a consistent C++ class hierarchy that maps various types of R objects (vectors, matrices, functions, environments, . . . ) to dedicated C++ classes. Object interchange between R and C++ is managed by simple, flexible and extensible concepts which include broad support for C++ Standard Template Library idioms. C++ code can both be compiled, linked and loaded on the fly, or added via packages. Flexible error and exception code handling is provided. Rcpp substantially lowers the barrier for programmers wanting to combine C++ code with R.},
   issn = {1548-7660},
   pages = {1--18},
   doi = {10.18637/jss.v040.i08},
   url = {https://www.jstatsoft.org/v040/i08}
}