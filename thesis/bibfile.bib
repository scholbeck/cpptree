
@book{quinlan_c45,
author = {Quinlan, J. Ross},
title = {C4.5: Programs for Machine Learning},
year = {1993},
isbn = {1558602402},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {From the Publisher:Classifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. C4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies.This book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses.}
}

@BOOK{cart_1,
  author        = {L. Breiman and J. Friedman and R. Olshen and C. Stone},
  title         = {{Classification and Regression Trees}},
  publisher     = {Wadsworth and Brooks},
  address       = {Monterey, CA},
  year          = {1984},
  note          = {new edition \cite{cart93}?},
  remarks       = {cited in \cite{cslu:esca98mm, cslu:icslp98cronk, cstr:unitsel97} for CART, clustering, and decision trees},
  abstract      = {},
}

@BOOK{cart_2,
  author        = {Leo {Breiman} and J. H. {Friedman} and R. A. {Olshen} and C. J. {Stone}},
  title         = {Classification and Regression Trees},
  year          = {1984},
  publisher     = {Wadsworth Publishing Company},
  address       = {Belmont, California, U.S.A.},
  series        = {Statistics/Probability Series},
  isbn-hard     = {0534980546 (softcover)},
  isbn-soft     = {0534980538 (hardcover)},
}

@BOOK{cart_3,
  author        = {Leo Breiman and others},
  title         = {{Classification and Regression Trees}},
  publisher     = {Chapman \& Hall},
  address       = {New York},
  year          = {1984},
  pages         = {358},
  note          = {new edition of \cite{cart84}?},
  isbn          = {0-412-04841-8},
  url           = {http://www.crcpress.com/catalog/C4841.htm},
  amazon-url    = {http://www.amazon.de/exec/obidos/ASIN/0412048418},
  price         = {\$44.95, DM 83.26 EUR 42.57},
  remarks       = {\tbf},
  abstract      = {},
}

@book{hastie_elemstatlearn,
  added-at = {2008-05-16T16:17:42.000+0200},
  address = {New York, NY, USA},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  biburl = {https://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
  interhash = {d585aea274f2b9b228fc1629bc273644},
  intrahash = {f58afc5c9793fcc8ad8389824e57984c},
  keywords = {ml statistics},
  publisher = {Springer New York Inc.},
  series = {Springer Series in Statistics},
  timestamp = {2008-05-16T16:17:43.000+0200},
  title = {The Elements of Statistical Learning},
  year = 2001
}

@article{breiman_randomforests,
author = {Breiman, Leo},
title = {Random Forests},
year = {2001},
issue_date = {October 1 2001},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {45},
number = {1},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1010933404324},
doi = {10.1023/A:1010933404324},
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund &amp; R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
journal = {Mach. Learn.},
month = oct,
pages = {5–32},
numpages = {28},
keywords = {ensemble, regression, classification}
}

@article{zeileis_mob,
author = {Achim Zeileis and Torsten Hothorn and Kurt Hornik},
title = {Model-Based Recursive Partitioning},
journal = {Journal of Computational and Graphical Statistics},
volume = {17},
number = {2},
pages = {492-514},
year  = {2008},
publisher = {Taylor & Francis},
doi = {10.1198/106186008X319331},

URL = { 
        https://doi.org/10.1198/106186008X319331
    
},
eprint = { 
        https://doi.org/10.1198/106186008X319331
    
}

}

@Article{party_package,
title = {Unbiased Recursive Partitioning: A Conditional Inference
  Framework},
author = {Torsten Hothorn and Kurt Hornik and Achim Zeileis},
journal = {Journal of Computational and Graphical Statistics},
year = {2006},
volume = {15},
number = {3},
pages = {651--674},
}

@Manual{r_citation,
title = {R: A Language and Environment for Statistical Computing},
author = {{R Core Team}},
organization = {R Foundation for Statistical Computing},
address = {Vienna, Austria},
year = {2020},
url = {https://www.R-project.org/},
}

@Article{partykit_package,
title = {{partykit}: A Modular Toolkit for Recursive Partytioning
  in {R}},
author = {Torsten Hothorn and Achim Zeileis},
journal = {Journal of Machine Learning Research},
year = {2015},
volume = {16},
pages = {3905-3909},
url = {https://jmlr.org/papers/v16/hothorn15a.html},
}

@Article{ranger_package,
title = {{ranger}: A Fast Implementation of Random Forests for High
  Dimensional Data in {C++} and {R}},
author = {Marvin N. Wright and Andreas Ziegler},
journal = {Journal of Statistical Software},
year = {2017},
volume = {77},
number = {1},
pages = {1--17},
doi = {10.18637/jss.v077.i01},
}

@article{eddelbuettel_rcpp,
   author = {Dirk Eddelbuettel and Romain Francois},
   title = {Rcpp: Seamless R and C++ Integration},
   journal = {Journal of Statistical Software, Articles},
   volume = {40},
   number = {8},
   year = {2011},
   keywords = {},
   abstract = {The Rcpp package simplifies integrating C++ code with R. It provides a consistent C++ class hierarchy that maps various types of R objects (vectors, matrices, functions, environments, . . . ) to dedicated C++ classes. Object interchange between R and C++ is managed by simple, flexible and extensible concepts which include broad support for C++ Standard Template Library idioms. C++ code can both be compiled, linked and loaded on the fly, or added via packages. Flexible error and exception code handling is provided. Rcpp substantially lowers the barrier for programmers wanting to combine C++ code with R.},
   issn = {1548-7660},
   pages = {1--18},
   doi = {10.18637/jss.v040.i08},
   url = {https://www.jstatsoft.org/v040/i08}
}

@Article{quinlan_induction,
author={Quinlan, J. R.},
title={Induction of decision trees},
journal={Machine Learning},
year={1986},
month={Mar},
day={01},
volume={1},
number={1},
pages={81-106},
abstract={The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.},
issn={1573-0565},
doi={10.1007/BF00116251},
url={https://doi.org/10.1007/BF00116251}
}


@article{loh_trees_review,
author = {Loh, Wei-Yin},
title = {Fifty Years of Classification and Regression Trees},
journal = {International Statistical Review},
volume = {82},
number = {3},
pages = {329-348},
keywords = {Classification trees, regression trees, machine learning, prediction},
doi = {https://doi.org/10.1111/insr.12016},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12016},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12016},
abstract = {AbstractFifty years have passed since the publication of the first regression tree algorithm. New techniques have added capabilities that far surpass those of the early methods. Modern classification trees can partition the data with linear splits on subsets of variables and fit nearest neighbor, kernel density, and other models in the partitions. Regression trees can fit almost every kind of traditional statistical model, including least-squares, quantile, logistic, Poisson, and proportional hazards models, as well as models for longitudinal and multiresponse data. Greater availability and affordability of software (much of which is free) have played a significant role in helping the techniques gain acceptance and popularity in the broader scientific community. This article surveys the developments and briefly reviews the key ideas behind some of the major algorithms.},
year = {2014}
}


﻿@Article{ali_trees,
author={Ali, M. Abid
and Hickman, P. J.
and Clementson, A. T.},
title={The Application of Automatic Interaction Detection (AID) in Operational Research},
journal={Operational Research Quarterly (1970-1977)},
year={1975},
month={2021/06/14/},
publisher={Palgrave Macmillan Journals},
volume={26},
number={2},
pages={243-252},
abstract={[Automatic Interaction Detection (AID) is a multivariate statistical technique. This paper describes the algorithm and illustrates its application to a particular case study in the British fishing industry.]},
note={Full publication date: Jun., 1975},
issn={00303623},
doi={10.2307/3008458},
url={https://doi.org/10.2307/3008458},
url={http://www.jstor.org/stable/3008458}
}

@article{morgan_sonquist_trees,
author = { James N.   Morgan  and  John A.   Sonquist },
title = {Problems in the Analysis of Survey Data, and a Proposal},
journal = {Journal of the American Statistical Association},
volume = {58},
number = {302},
pages = {415-434},
year  = {1963},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.1963.10500855},

URL = { 
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1963.10500855
    
},
eprint = { 
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1963.10500855
    
}

}

@inbook{hawkins_AID, place={Cambridge}, title={AUTOMATIC INTERACTION DETECTION}, DOI={10.1017/CBO9780511897375.006}, booktitle={Topics in Applied Multivariate Analysis}, publisher={Cambridge University Press}, author={Hawkins, Douglas M. and Kass, Gordon V. and Hawkins, D. M.}, year={1982}, pages={269–302}}

@article{podgorelec_trees_medicine,
author = {Podgorelec, Vili and Kokol, Peter and Stiglic, Bruno and Rozman, Ivan},
year = {2002},
month = {11},
pages = {445-63},
title = {Decision Trees: An Overview and Their Use in Medicine},
volume = {26},
journal = {Journal of medical systems},
doi = {10.1023/A:1016409317640}
}

@article{hothorn_ctree,
author = {Torsten Hothorn and Kurt Hornik and Achim Zeileis},
title = {Unbiased Recursive Partitioning: A Conditional Inference Framework},
journal = {Journal of Computational and Graphical Statistics},
volume = {15},
number = {3},
pages = {651-674},
year  = {2006},
publisher = {Taylor & Francis},
doi = {10.1198/106186006X133933},

URL = { 
        https://doi.org/10.1198/106186006X133933
    
},
eprint = { 
        https://doi.org/10.1198/106186006X133933
    
}

}

@misc{schaaf_surrogate_tree,
      title={Enhancing Decision Tree based Interpretation of Deep Neural Networks through L1-Orthogonal Regularization}, 
      author={Nina Schaaf and Marco F. Huber and Johannes Maucher},
      year={2019},
      eprint={1904.05394},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{ram_density_estimation_tree,
author = {Ram, Parikshit and Gray, Alexander G.},
title = {Density Estimation Trees},
year = {2011},
isbn = {9781450308137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020408.2020507},
doi = {10.1145/2020408.2020507},
abstract = {In this paper we develop density estimation trees (DETs), the natural analog of classification trees and regression trees, for the task of density estimation. We consider the estimation of a joint probability density function of a d-dimensional random vector X and define a piecewise constant estimator structured as a decision tree. The integrated squared error is minimized to learn the tree. We show that the method is nonparametric: under standard conditions of nonparametric density estimation, DETs are shown to be asymptotically consistent. In addition, being decision trees, DETs perform automatic feature selection. They empirically exhibit the interpretability, adaptability and feature selection properties of supervised decision trees while incurring slight loss in accuracy over other nonparametric density estimators. Hence they might be able to avoid the curse of dimensionality if the true density is sparse in dimensions. We believe that density estimation trees provide a new tool for exploratory data analysis with unique capabilities.},
booktitle = {Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {627–635},
numpages = {9},
keywords = {density estimation, decision trees},
location = {San Diego, California, USA},
series = {KDD '11}
}

@INPROCEEDINGS{quinlan_model_tree,
author = {J. R. Quinlan},
title = {Learning With Continuous Classes},
booktitle = {},
year = {1992},
pages = {343--348},
publisher = {World Scientific}
}

@article{messenger_mandell_thaid,
  title={A Modal Search Technique for Predictive Nominal Scale Multivariate Analysis},
  author={R. C. Messenger and Lewis Mandell},
  journal={Journal of the American Statistical Association},
  year={1972},
  volume={67},
  pages={768-772}
}

@article{fisher_lda,
author = {FISHER, R. A.},
title = {THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS},
journal = {Annals of Eugenics},
volume = {7},
number = {2},
pages = {179-188},
doi = {https://doi.org/10.1111/j.1469-1809.1936.tb02137.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1936.tb02137.x},
abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
year = {1936}
}

@Article{potts_incremental_model_tree,
author={Potts, Duncan
and Sammut, Claude},
title={Incremental Learning of Linear Model Trees},
journal={Machine Learning},
year={2005},
month={Nov},
day={01},
volume={61},
number={1},
pages={5-48},
abstract={A linear model tree is a decision tree with a linear functional model in each leaf. Previous model tree induction algorithms have been batch techniques that operate on the entire training set. However there are many situations when an incremental learner is advantageous. In this article a new batch model tree learner is described with two alternative splitting rules and a stopping rule. An incremental algorithm is then developed that has many similarities with the batch version but is able to process examples one at a time. An online pruning rule is also developed. The incremental training time for an example is shown to only depend on the height of the tree induced so far, and not on the number of previous examples. The algorithms are evaluated empirically on a number of standard datasets, a simple test function and three dynamic domains ranging from a simple pendulum to a complex 13 dimensional flight simulator. The new batch algorithm is compared with the most recent batch model tree algorithms and is seen to perform favourably overall. The new incremental model tree learner compares well with an alternative online function approximator. In addition it can sometimes perform almost as well as the batch model tree algorithms, highlighting the effectiveness of the incremental implementation.},
issn={1573-0565},
doi={10.1007/s10994-005-1121-8},
url={https://doi.org/10.1007/s10994-005-1121-8}
}

@article{kass_chaid,
  title={An Exploratory Technique for Investigating Large Quantities of Categorical Data},
  author={G. V. Kass},
  journal={Journal of The Royal Statistical Society Series C-applied Statistics},
  year={1980},
  volume={29},
  pages={119-127}
}


@Article{brodley_multivariate_trees,
author={Brodley, Carla E.
and Utgoff, Paul E.},
title={Multivariate Decision Trees},
journal={Machine Learning},
year={1995},
month={Apr},
day={01},
volume={19},
number={1},
pages={45-77},
abstract={Unlike a univariate decision tree, a multivariate decision tree is not restricted to splits of the instance space that are orthogonal to the features' axes. This article addresses several issues for constructing multivariate decision trees: representing a multivariate test, including symbolic and numeric features, learning the coefficients of a multivariate test, selecting the features to include in a test, and pruning of multivariate decision trees. We present several new methods for forming multivariate decision trees and compare them with several well-known methods. We compare the different methods across a variety of learning tasks, in order to assess each method's ability to find concise, accurate decision trees. The results demonstrate that some multivariate methods are in general more effective than others (in the context of our experimental assumptions). In addition, the experiments confirm that allowing multivariate tests generally improves the accuracy of the resulting decision tree over a univariate tree.},
issn={1573-0565},
doi={10.1023/A:1022607123649},
url={https://doi.org/10.1023/A:1022607123649}
}

@inproceedings{norouzi_nongreedy_tree,
author = {Norouzi, Mohammad and Collins, Maxwell D. and Johnson, Matthew and Fleet, David J. and Kohli, Pushmeet},
title = {Efficient Non-Greedy Optimization of Decision Trees},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Decision trees and randomized forests are widely used in computer vision and machine learning. Standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria. This greedy procedure often leads to suboptimal trees. In this paper, we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters, based on a global objective. We show that the problem of finding optimal linear-combination (oblique) splits for decision trees is related to structured prediction with latent variables, and we formulate a convex-concave upper bound on the tree's empirical loss. Computing the gradient of the proposed surrogate objective with respect to each training exemplar is O(d2), where d is the tree depth, and thus training deep trees is feasible. The use of stochastic gradient descent for optimization enables effective training with large datasets. Experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1729–1737},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{chow_inequality_test,
title={Tests of equality between sets of coefficients in two linear regressions (econometrics voi 28},
author={G. Chow},
year={1960}
}

@Article{derose_survival_tree,
author={De Rose, Alessandra
and Pallara, Alessandro},
title={Survival Trees: An Alternative Non-Parametric Multivariate Technique for Life History Analysis},
journal={European Journal of Population / Revue europ{\'e}enne de D{\'e}mographie},
year={1997},
month={Sep},
day={01},
volume={13},
number={3},
pages={223-241},
abstract={In this paper an extension of tree-structured methodology to cover censored survival analysis is discussed. Tree-based methods (also called recursive partitioning) provide a useful alternative to the classical survival data analysis techniques, such as the semi-parametric model of Cox, whenever the main purpose is defining groups of individuals, either with complete or censored life history, having different survival probability, based on the values of selected covariates. The essential feature of recursive partitioning is the construction of a decision rule in the form of a binary tree. Trees generally require fewer assumptions than classical methods and handle non standard and non linear data structures efficiently. Tree-growing methods make the processes of covariate selection and grouping of categories in event history models explicit. An example concerning the analysis of time to marriage of Italian women is presented.},
issn={1572-9885},
doi={10.1023/A:1005844818027},
url={https://doi.org/10.1023/A:1005844818027}
}

@article{mingers_pruning,
author = {Mingers, John},
year = {1989},
month = {01},
pages = {227-243},
title = {An Empirical Comparison of Pruning Methods for Decision Tree Induction},
volume = {4},
journal = {Machine Learning},
doi = {10.1023/A:1022604100933}
}

@article{quinlan_simplifying,
title = {Simplifying decision trees},
journal = {International Journal of Man-Machine Studies},
volume = {27},
number = {3},
pages = {221-234},
year = {1987},
issn = {0020-7373},
doi = {https://doi.org/10.1016/S0020-7373(87)80053-6},
url = {https://www.sciencedirect.com/science/article/pii/S0020737387800536},
author = {J.R. Quinlan},
abstract = {Many systems have been developed for constructing decision trees from collections of examples. Although the decision trees generated by these methods are accurate and efficient, they often suffer the disadvantage of excessive complexity and are therefore incomprehensible to experts. It is questionable whether opaque structures of this kind can be described as knowledge, no matter how well they function. This paper discusses techniques for simplifying decision trees while retaining their accuracy. Four methods are described, illustrated, and compared on a test-bed of decision trees from a variety of domains.}
}

@article{razi_cart_comparison,
title = {A comparative predictive analysis of neural networks (NNs), nonlinear regression and classification and regression tree (CART) models},
journal = {Expert Systems with Applications},
volume = {29},
number = {1},
pages = {65-74},
year = {2005},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2005.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0957417405000072},
author = {Muhammad A. Razi and Kuriakose Athappilly},
keywords = {Neural networks, Regression, Classification and regression tree},
abstract = {Numerous articles comparing performances of statistical and Neural Networks (NNs) models are available in the literature, however, very few involved Classification and Regression Tree (CART) models in their comparative studies. We perform a three-way comparison of prediction accuracy involving nonlinear regression, NNs and CART models using a continuous dependent variable and a set of dichotomous and categorical predictor variables. A large dataset on smokers is used to run these models. Different prediction accuracy measuring procedures are used to compare performances of these models. The outcomes of predictions are discussed and the outcomes of this research are compared with the results of similar studies.}
}

@ARTICLE{ciampi_glm_tree,
title = {Generalized regression trees},
author = {Ciampi, Antonio},
year = {1991},
journal = {Computational Statistics & Data Analysis},
volume = {12},
number = {1},
pages = {57-78},
url = {https://EconPapers.repec.org/RePEc:eee:csdana:v:12:y:1991:i:1:p:57-78}
}

@article{loh_guide,
author = {Loh, Wei-Yin},
year = {2002},
month = {04},
pages = {361-386},
title = {Regression Trees With Unbiased Variable Selection and Interaction Detection},
volume = {12},
journal = {Statistica Sinica}
}

﻿@Article{gama_functional_tree,
author={Gama, Jo{\~a}o},
title={Functional Trees},
journal={Machine Learning},
year={2004},
month={Jun},
day={01},
volume={55},
number={3},
pages={219-250},
abstract={In the context of classification problems, algorithms that generate multivariate trees are able to explore multiple representation languages by using decision tests based on a combination of attributes. In the regression setting, model trees algorithms explore multiple representation languages but using linear models at leaf nodes. In this work we study the effects of using combinations of attributes at decision nodes, leaf nodes, or both nodes and leaves in regression and classification tree learning. In order to study the use of functional nodes at different places and for different types of modeling, we introduce a simple unifying framework for multivariate tree learning. This framework combines a univariate decision tree with a linear function by means of constructive induction. Decision trees derived from the framework are able to use decision nodes with multivariate tests, and leaf nodes that make predictions using linear functions. Multivariate decision nodes are built when growing the tree, while functional leaves are built when pruning the tree. We experimentally evaluate a univariate tree, a multivariate tree using linear combinations at inner and leaf nodes, and two simplified versions restricting linear combinations to inner nodes and leaves. The experimental evaluation shows that all functional trees variants exhibit similar performance, with advantages in different datasets. In this study there is a marginal advantage of the full model. These results lead us to study the role of functional leaves and nodes. We use the bias-variance decomposition of the error, cluster analysis, and learning curves as tools for analysis. We observe that in the datasets under study and for classification and regression, the use of multivariate decision nodes has more impact in the bias component of the error, while the use of multivariate decision leaves has more impact in the variance component.},
issn={1573-0565},
doi={10.1023/B:MACH.0000027782.67192.13},
url={https://doi.org/10.1023/B:MACH.0000027782.67192.13}
}

@article{karalic_retis,
author = {Karalic, Aram},
year = {2000},
month = {05},
pages = {},
title = {Employing Linear Regression in Regression Tree Leaves}
}

@article{grimshaw_treed,
 ISSN = {10618600},
 URL = {http://www.jstor.org/stable/1390778},
 abstract = {Given a data set consisting of n observations on p independent variables and a single dependent variable, treed regression creates a binary tree with a simple linear regression function at each of the leaves. Each node of the tree consists of an inequality condition on one of the independent variables. The tree is generated from the training data by a recursive partitioning algorithm. Treed regression models are more parsimonious than CART models because there are fewer splits. Additionally, monotonicity in some or all of the variables can be imposed.},
 author = {William P. Alexander and Scott D. Grimshaw},
 journal = {Journal of Computational and Graphical Statistics},
 number = {2},
 pages = {156--175},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
 title = {Treed Regression},
 volume = {5},
 year = {1996}
}
